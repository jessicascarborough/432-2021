---
title: "432 Lab 02 Template"
author: "Your Name Goes Here"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    self_contained: true
    number_sections: true
    code_folding: show
    df_print: paged
---

## Setup {-}

```{r setup, message = FALSE}
library(knitr)
library(rmdformats)

## Global options
opts_chunk$set(comment=NA)
opts_knit$set(width=75)
```

## Package Loading {-}

```{r load_packages, message = FALSE}
library(here)
library(magrittr)
library(janitor)
library(scales)    # assists with axis labels in ggplot
library(naniar)    # for missing value tools
library(patchwork) # for plot organization 
library(broom)     # for tidying model output
# add other packages here as desired

library(tidyverse)
```

## Import the `oh_counties_2020` Data {-}

This template assumes that you've placed the `oh_counties_2020.csv` data set in a sub-folder called `data` beneath the `lab02` project directory you're using in R Studio.

```{r, message = FALSE}
oh20 <- read_csv(here("data", "oh_counties_2020.csv")) 

oh20 <- oh20 %>%
    clean_names() %>%
    mutate(fips = as.character(fips))


oh20
```

# Question 1

For this visualization, we're going to look at the relationship between a county's unemployment rate in 16+ individuals and access to healthy foods, assessing older and younger populations separately. In order to do so, we will first extract our variables of interest from the `oh_counties_2020` data set, selecting `fips`, `unemployed`, and `age65plus`. 

```{r q01_select_vars}

q01 <- oh20 %>%
  select(fips, food_env, unemployed, age65plus)

```


Next, we'd like to split the age65plus variable, which represents the % of the population who are 65 and over, into a binary variable representing counties with populations that are younger or older. There are many ways to choose a cutoff point, but first we'll take a quick look at the distribution of this variable to see if there's any obvious bimodel distribution. Based on the histogram below, this variable does not have an obvious cutoff value (e.g. between two bimodal peaks). Therefore, we'll choose the median as a simple cutoff value, ensuring that both groups have approximately the same number of samples (counties). 


```{r q01_assess_cutoff_age}
g <- ggplot(q01, aes(x = age65plus)) +
  geom_histogram(fill = "lightsteelblue3", col = "white", binwidth = 1) + 
  labs(x = "% of population who are 65 and over", y = "Count",
       title = "Distribution of age variable demonstrates no obvious cutoff value") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

g

```

Now that we know our cutoff value (the median of the `age65plus` variable), we'll use `mutate` to create a new variable called `age65plus_fct`. This variable will denote counties where `age65plus` is greater than or equal to the median of `age65plus` as `Older Community`, while counties less than or equal to the median of `age65plus` are denoted as `Younger Community`. Additionally, we'll order this factor from younger to older for the purpose of our visualization. 


```{r q01_clean_data}

q01 <- q01 %>%
  mutate(age65plus_fct = 
           case_when(age65plus < median(age65plus) ~ "Younger Community",
                     age65plus >= median(age65plus) ~ "Older Community")) %>%
  mutate(age65plus_fct = factor(age65plus_fct, levels = c("Younger Community", "Older Community")))

```

Finally, we'll produce our visualization using `ggplot`, comparing `food_env` and `unemployed` with as scatter plots faceted by the `age65plus_fct` variable. 

```{r q01_data_viz}

caption_wrap <- str_wrap("Using the 2020 County Health Rankings data for Ohio, we plot a point for each county, indicating the population percentage (16+) who is unemployed and looking for work and a metric denoting access to healthy foods, with counties separated by younger and older populations. Counties with increased unemployment tend to have decreased access to healthy foods. This trend is true in both younger and older communities.")

g2 <- ggplot(q01, aes(x = unemployed, y = food_env, color = age65plus_fct)) + 
  geom_point(aes(size = 4, alpha = 0.5), show.legend = FALSE) + 
  facet_wrap(~age65plus_fct) + 
  labs(x = "Unemployment Rate",
       y = "Access to Healthy Foods\n(0-10 from worst to best)", 
       title = "Increased unemployment correlates to decreased access to healthy foods", 
       subtitle = "As seen in both younger and older Ohio counties", 
       caption = caption_wrap) +
  scale_x_continuous(labels = percent_format(scale = 1)) + 
  scale_color_manual(values = c("lightsteelblue3", "lightsteelblue4")) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5))

g2

```

# Question 2

In academia, there is perhaps no truer phrase than "Publish or perish." Simply put, if an investigator is unable to publish their work in respected journals, they will have a difficult time achieving conventional career milestones (e.g. grant funding, employment at a prestigious university, tenure, etc.). When one examines the typical requirements for having an article accepted for publication, it becomes clear why so many scientists are "desperately seeking signal" from their data. Journals typically assess manuscripts not only on experimental design, but also on the novelty of the results. In my work, this pressure is certainly present when producing predictive models. Our hopes of finding a signal from noisy data can lead to complex models with too many parameters which overfit the data. In *The Signal and the Noise*, Nate Silver explains, "Overfitting represents a double whammy: it makes our model look better on paper but perform worse in the real world" (Silver, Chapter 6). He aptly points out that overfit models may look ``better'' than properly fit models using traditional metrics, such as r-squared. However, my passion for research is based in translational science--that is, science which aims to move discoveries from the laboratory bench (or computer) to clinical care. With my future work, these principles will help me remember that although overfit models may provide a short term gain (easier publications), they will hinder our ability to help patients in the long run.  


# Question 3

In this question, we'll create a linear regression model to assess the impact of food availability on obesity rates in a county, taking into account median income. To start, we'll first extract these variables of interest, check for missing values, and then assess each variable's distribution using a histogram. 

```{r q03_data_check}

q03 <- oh20 %>%
  select(fips, county, obese_pct, food_env, median_income)

miss_case_table(q03)


```

As we can see there are no missing values, so we'll move forward with plotting the distribution of each variable. As we see below, none of the variables are egregiously non-normal, so we will move forward without transforming any of them.


```{r q03_data_viz}

g_obese_pct <- ggplot(q03, aes(x = obese_pct)) + 
  geom_histogram(fill = "lightsteelblue2", col = "white", binwidth = 1) + 
  labs(x = "% of adults with BMI > 30", y = "Count",
       title = "Distribution of \nObesity Rates") +
  scale_x_continuous(labels = percent_format(scale = 1)) + 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

g_food_env <- ggplot(q03, aes(x = food_env)) + 
  geom_histogram(fill = "lightsteelblue3", col = "white", binwidth = 0.25) + 
  labs(x = "Indicator Metric \n(0-10 from worst to best)", y = "Count",
       title = "Distribution of \nHealthy Food Access") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

g_median_income <- ggplot(q03, aes(x = median_income)) + 
  geom_histogram(fill = "lightsteelblue4", col = "white", binwidth = 4000) + 
  labs(x = "Estimated Median Income", y = "Count",
       title = "Distribution of \nEstimated Median Income") +
  scale_x_continuous(labels = dollar_format()) + 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))


g_obese_pct + g_food_env + g_median_income

```


Finally, we'll produce our linear regression model with `obese_pct` as our outcome variable and `food_env` as our predictor variable, adjusting for `median_income`. In this model, we see that the predicted percentage of adults with a BMI over 30 increases by 1.40% for every unit increase in the `food_env` metric. We are 90% confident that this predicted increase falls between 0.27% and 2.53%. 

```{r q03_linear_regr}

m1_q03 <- lm(obese_pct ~ food_env + median_income, q03)

tidy(m1_q03, conf.int = TRUE, conf.level = 0.90) %>%
    select(term, estimate, 
           low90 = conf.low, high90 = conf.high, 
           se = std.error, p = p.value) %>%
    kable(digits = c(0,5,5,5,5,5))

```

# Question 4

Evaluate the quality of the model you fit in Question 3 in terms of adherence to regression modeling assumptions, through the specification and written evaluation of residual plots. What might be done to improve the fit of the model youâ€™ve developed in Question 3? Identify by name any outlying counties and explain why they are flagged as outliers.

Next, we'll examine the residual plots for the model produced in Question 3, `m1_q03`. 

Based on the plots below, our model adheres to the principles of regression modeling assumptions well. The Residuals vs. Fitted plot does not have any obvious pattern or bend to the curve, indicating acceptably constant variance. Similarly, the Scale-Location plot does not indicate non-constant variance, as the loess smooth curve is relatively flat. The Residuals vs. Fitted plot shows us that the counties from rows 43, 46, and 68 are the biggest outliers in the data, as they have the three largest residuals. However, based on the Residuals vs. Leverage plot, they aren't influencing the model very much. Additionally, the Normal Q-Q plot does not demonstrate substantial non-normality. 

Finally, to improve this model, we could remove these outlying counties, although their lack of influence on the model likely means it will not effect the model greatly. We will identify the names of these counties below. 

```{r q04_residual_plots}

par(mfrow = c(2,2))
plot(m1_q03)
par(mfrow = c(1,1))

```

Here, we'll use the `slice` function to identify the greatest outliers based on the row numbers identified in the Residuals vs. Fitted plot. Those counties are Lake, Logan, and Preble county. Based on the plot Logan County (row 46) was under-predicted by our model, while Lake County and Preble County were over-predicted by our model.


```{r q04_outliers}

slice(q03, c(43, 46, 68))

```

# Question 5

Here, we're going to assess whether severe housing problems (`sev_housing`) or daily pollution exposure (`pm2_5`) are related to the presence of a water violation (`h2oviol`) in Ohio counties. We'll start by extracting these variables of interest and check for missing values. Using the `miss_case_table` function from the `naniar` package, we see that there are no missing values. 


```{r q05_data_check}

q05 <- oh20 %>%
  select(fips, county, sev_housing, pm2_5, h2oviol)

miss_case_table(q05)

```


Next, we'll use `geom_point` in `ggplot` to assess the relationship of `sev_housing` and `h2oviol` and the relationship of `pm2_5` and `h2oviol`. Based on the plots below, there isn't an obvious relationship between `sev_housing` and `h2oviol` or `pm2_5` and `h2oviol`. We'll move on to building the logistic regression model. 


```{r q05_data_viz}


g_housing_water <- ggplot(q05, aes(x = sev_housing, y = h2oviol)) + 
  geom_point() +
  labs(x = "Percentage of Households with a Severe Housing Problem",
       y = "Presence of a water violation",
       title = "Housing Problems vs. Water Violations in Ohio Counties") +
  scale_x_continuous(labels = percent_format(scale = 1)) + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

g_pm2_5_water <- ggplot(q05, aes(x = pm2_5, y = h2oviol)) + 
  geom_point() +
  labs(x = bquote('Avg Daily Fine Particulate Matter (\u00B5g/'*m^2*')'),
       y = "Presence of a water violation",
       title = "Pollution vs. Water Violations in Ohio Counties")+
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


g_housing_water
g_pm2_5_water

```

Below, we'll use the `glm` function to predict `h2oviol` as a factor, using `sev_housing` and `pm2_5` as predictors. We'll then assess the exponentiated coefficients using `tidy`, and further, we'll look at the 90% uncertainty interval for these coefficients using  `confint` (again, exponentiating those results).


The odds ratio between two counties whose severe housing percentage differs by 1% is 0.953. In other words, our model estimates that a county with 1% more of their population living with severe housing problems (`sev_housing`) will have 95.3% of the odds of having water violation (`h20viol`) compared to a county without that increased percentage of the population living with severe housing problems.  We are 90% confident that this odds ratio falls between 0.80 and 1.12. Because this uncertainty interval clearly overlaps with 1, we cannot assert that changes in the population living with severe housing problems are associated with statistically detectable changes in the odds ratio. 


```{r q05_log_regr}

m1_q05 <- glm(factor(h2oviol) ~ sev_housing + pm2_5, data = q05,
            family = binomial(link = "logit"))

tidy(m1_q05, exponentiate = TRUE) %>%
  select(term, estimate)

exp(confint(m1_q05, level = 0.90))

```


# Session Information {-}

```{r}
xfun::session_info()
```